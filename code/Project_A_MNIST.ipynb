{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "## 1st Programming Exercise\n",
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$softmax(z) = \\frac{ e^{z} }{ \\sum_{i=1}^{k} e^{ z_{i} } }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function (logLikelihood plus reguralization term) we want to maximize for the problem of classifying N number of data in K categories/classes is:\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}||^2, \n",
    "$$\n",
    "\n",
    "where $y_{nk}$ is the softmax function defined as:\n",
    "\n",
    "$$y_{nk} = \\frac{e^{\\mathbf{w_{(2)}}_k^T \\mathbf{z}_n}}{\\sum_{j=1}^K e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n}}$$\n",
    "$W$ is a $K \\times (D+1)$ matrix where each line represents the vector $\\mathbf{w}_k$.\n",
    "\n",
    "\n",
    "The cost function can be simplified in the following form:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w_{(2)}}_k^T \\mathbf{z}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}||^2, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In the above formula we have used the fact that $\\sum_{k=1}^K t_{nk} = 1$. \n",
    "\n",
    "The partial derrivatives of this function are given by the following $K \\times (D+1)$ matrix:\n",
    "\n",
    "$$W2$$\n",
    "\n",
    "$$\n",
    "W2 : (T - S)^TZ - \\lambda W_{(2)},\n",
    "$$\n",
    "\n",
    "$$W1$$\n",
    "\n",
    "$$\n",
    "W1 : ((T - S)W_{(2)}H')^TX - \\lambda W_{(1)},\n",
    "$$\n",
    "\n",
    "\n",
    "where $T$ is an $N \\times K$ matrix with the truth values of the training data, such that $[T]_{nk} = t_{nk}$, $S$ is the corresponding $N \\times K$ matrix that holds the softmax probabilities such that $[S]_{nk} = y_{nk}$ and $X$ is the $N \\times (D + 1)$ matrix of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Activation Functions:\")\n",
    "print(\"1) h(α) = log(1 + e^α)\")\n",
    "print(\"2) h(α) = tanh(α)\")\n",
    "print(\"3) h(α) = cos(α)\")\n",
    "choice = input(\"What is your choice? (Type 1, 2 or 3) : \")\n",
    "choice = int(choice)\n",
    "print(choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activation Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_func(choice, A):\n",
    "\n",
    "    if choice == 1:\n",
    "        Z = np.log(1+np.exp(A))\n",
    "\n",
    "    elif choice == 2:\n",
    "        Z = np.tanh(A) \n",
    "    \n",
    "    elif choice == 3:\n",
    "        Z = np.cos(np.deg2rad(A))\n",
    "\n",
    "    else:\n",
    "        print(\"You typed an incorrect choice.\")\n",
    "        Z = \"Try again!\"\n",
    "        \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activation Functions Derivatives </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_der(choice, A):\n",
    "    \n",
    "    if choice == 1:\n",
    "        H = (np.exp(A))/(1+np.exp(A))\n",
    "    \n",
    "    elif choice == 2:\n",
    "        H = (1 - (np.tanh(A))**2) \n",
    "    \n",
    "    elif choice == 3:\n",
    "        H = (-np.sin(A)) \n",
    "        \n",
    "    else:\n",
    "        print(\"You typed an incorrect choice.\")\n",
    "        H = \"Try again!\"\n",
    "        \n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Softmax </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mnist Dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data folder there is the dataset of mnist. Mnists consists of $28x28$ grayscale images. In total there are 10 training files\n",
    "train0.txt, train1.txt, ..., train9.txt where each rows of train$k$.txt corresponds to an example that belongs to the class $k$.\n",
    "\n",
    "The testing data follows the same format.\n",
    "\n",
    "In total we have $6*10^4$ training examples and $10^3$ testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.to_numpy()\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.to_numpy()\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> View of the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 5 random images from the training set\n",
    "n = 100\n",
    "sqrt_n = int( n**0.5 )\n",
    "samples = np.random.randint(X_train.shape[0], size=n)\n",
    "\n",
    "plt.figure( figsize=(11,11) )\n",
    "\n",
    "cnt = 0\n",
    "for i in samples:\n",
    "    cnt += 1\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt )\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n",
    "    plt.imshow( X_train[i].reshape(28,28), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Normalize the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column of ones to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack( (np.ones((X_train.shape[0],1) ), X_train) )\n",
    "X_test = np.hstack( (np.ones((X_test.shape[0],1) ), X_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cost function\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w_{(2)}}_k^T \\mathbf{z}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}||^2, \n",
    "$$\n",
    "\n",
    "\n",
    "we use the logsumexp trick, where m is the maximum element:\n",
    "$$\n",
    "\\log \\sum_{i=1}^{n} e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n} \n",
    "= \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n +m -m}\\Bigl) \\\\ \n",
    "= \\log \\Bigr( \\sum_{i=1}^{n} e^m e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "= \\log \\Bigr( e^m \\sum_{i=1}^{n} e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "= \\log \\ e^m + \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "= m + \\log \\Bigr( \\sum_{i=1} e^{\\mathbf{w_{(2)}}_j^T \\mathbf{z}_n-m}  \\Bigl)\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W2 : (T - S)^TZ - \\lambda W_{(2)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "W1 : ((T - S)W_{(2)}H')^TX - \\lambda W_{(1)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_grad_softmax(W1, W2, X, Z, t, lamda):\n",
    "    \n",
    "    y = Z.dot(W2.T)\n",
    "    max_error = np.max(y, axis=1)\n",
    "    \n",
    "    s = softmax(y)\n",
    "    \n",
    "    # Compute the cost function to check convergence\n",
    "    # Using the logsumexp trick for numerical stability - lec8.pdf slide 43\n",
    "    Ew = np.sum(t * y) - np.sum(max_error) - \\\n",
    "        np.sum(np.log(np.sum(np.exp(y - np.array([max_error, ] * y.shape[1]).T), 1))) - \\\n",
    "        (0.5 * lamda) * (np.sum(np.square(W2))) \n",
    "\n",
    "    # calculate gradient\n",
    "    gradEw2 = (t - s).T.dot(Z) - lamda * W2\n",
    "    gradEw1 = (((np.multiply((t - s).dot(W2), (act_der(choice, Z)))).T).dot(X))[1:,:] - lamda * W1\n",
    "        \n",
    "    return Ew, gradEw1, gradEw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_train(t, X, lamda, W1_init, W2_init, options):\n",
    "    \"\"\"inputs :\n",
    "      t: N x K binary output data vector indicating the k classes\n",
    "      X: N x (D+1) input data vector with ones already added in the first column\n",
    "      lamda: the positive regularizarion parameter\n",
    "      W1_init: M x (D+1) dimensional vector of the initial values of the parameters for W1\n",
    "      W2_init: K x (M+1) dimensional vector of the initial values of the parameters for W2\n",
    "      options: options(1) is the maximum number of iterations\n",
    "               options(2) is the tolerance\n",
    "               options(3) is the learning rate eta\n",
    "    outputs :\n",
    "      w1: the trained M x (D+1) dimensional vector of the parameters\n",
    "      w2: the trained K x (M+1) dimensional vector of the parameters\"\"\"\n",
    "        \n",
    "    \n",
    "    W1 = W1_init\n",
    "    W2 = W2_init\n",
    "    \n",
    "\n",
    "    # Maximum number of iteration of gradient ascend\n",
    "    _iter = options[0]\n",
    "    # Tolerance\n",
    "    tol = options[1]\n",
    "    # Learning rate\n",
    "    eta = options[2]\n",
    "\n",
    "    Ewold = -np.inf\n",
    "    costs = []\n",
    "\n",
    "    Z = np.zeros((X.shape[0],W1.shape[0]))\n",
    "    Z = np.hstack( (np.ones((Z.shape[0],1) ), Z) )\n",
    "        \n",
    "    \n",
    "    for i in range( 1, _iter+1 ):\n",
    "        \n",
    "        Z[:,1:] = act_func(choice, X.dot(W1.T))    \n",
    "\n",
    "        \n",
    "        Ew, gradEw1, gradEw2 = cost_grad_softmax(W1, W2, X, Z, t, lamda)\n",
    "        \n",
    "        # save cost\n",
    "        costs.append(Ew)\n",
    "        # Show the current cost function on screen\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration : %d, Cost function :%f' % (i, Ew))\n",
    "            \n",
    "        # Break if you achieve the desired accuracy in the cost function\n",
    "        if np.abs(Ew - Ewold) < tol:\n",
    "            break\n",
    "                        \n",
    "        # Update parameters based on gradient ascend\n",
    "        W1 = W1 + eta * gradEw1\n",
    "        W2 = W2 + eta * gradEw2\n",
    "        \n",
    "        Ewold = Ew\n",
    "    \n",
    "    return W1, W2, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking\n",
    "\n",
    "\n",
    "During gradient ascent/descent we compute the gradients $\\frac{\\partial E}{\\partial w}$, where $w$ denotes the parameters of the model.\n",
    "\n",
    "In order to make sure that these gradients are correct we will compare the exact gradients(that we have coded) with numerical estimates obtained by finite differences, you can use your code for computing $E$ to verify the code for computing $\\frac{\\partial E}{\\partial w}$.\n",
    "    Let's look back at the definition of a derivative (or gradient):\n",
    "    \n",
    "$$ \\frac{\\partial E}{\\partial w} = \\lim_{\\varepsilon \\to 0} \\frac{E(w + \\varepsilon) - E(w - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$  \n",
    "\n",
    "We know the following: \n",
    "- $\\frac{\\partial E}{\\partial w}$ is what you want to make sure you're computing correctly. ,\n",
    "- You can compute $E(w + \\varepsilon)$ and $E(w - \\varepsilon)$ (in the case that $w$ is a real number), since you're confident your implementation for $E$ is correct.\n",
    "\n",
    "Let's use equation (1) and a small value ( around $10^-4$ or $10^-6$, much smaller values could lead to numerical issues )for $\\varepsilon$ to make sure that your code for computing  $\\frac{\\partial E}{\\partial w}$ is correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_softmax(W1, W2, X, t, lamda):\n",
    "    \n",
    "    W1 = np.random.rand(*W1.shape)\n",
    "    W2 = np.random.rand(*W2.shape)\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    Z = np.zeros((X.shape[0],W1.shape[0]))\n",
    "    Z = np.hstack( (np.ones((Z.shape[0],1) ), Z) )\n",
    "    Z[:,1:] = act_func(choice, X.dot(W1.T))\n",
    "    z_sample= np.array(Z[_list, :])\n",
    "    \n",
    "    Ew, gradEw1, gradEw2 = cost_grad_softmax(W1, W2, x_sample, z_sample, t_sample, lamda)\n",
    "    #Ew, gradEw = cost_grad_softmax(W, x_sample, x_sample, lamda)\n",
    "    \n",
    "    print( \"gradEw1 shape: \", gradEw1.shape )\n",
    "    print( \"gradEw2 shape: \", gradEw2.shape )\n",
    "    \n",
    "    numericalGrad1 = np.zeros(gradEw1.shape)\n",
    "    numericalGrad2 = np.zeros(gradEw2.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad1.shape[0]):\n",
    "        for d in range(numericalGrad1.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w1[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus, _, _= cost_grad_softmax(w_tmp,W2, x_sample,z_sample, t_sample, lamda)\n",
    "\n",
    "            #subtract epsilon to the w1[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus, _, _= cost_grad_softmax(w_tmp,W2, x_sample,z_sample, t_sample, lamda)\n",
    "            \n",
    "            #approximate gradient ( E[ w1[k,d] + theta ] - E[ w1[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad1[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "            \n",
    "    for k in range(numericalGrad2.shape[0]):\n",
    "        for d in range(numericalGrad2.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w2[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus, _, _= cost_grad_softmax(W1,w_tmp, x_sample,z_sample, t_sample, lamda)\n",
    "\n",
    "            #subtract epsilon to the w2[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus, _, _= cost_grad_softmax(W1,w_tmp, x_sample,z_sample, t_sample, lamda)\n",
    "            \n",
    "            #approximate gradient ( E[ w2[k,d] + theta ] - E[ w2[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad2[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    \n",
    "    return  gradEw1, gradEw2, numericalGrad1, numericalGrad2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N of X\n",
    "N, D = X_train.shape\n",
    "\n",
    "# value of hidden units M\n",
    "#M = 100\n",
    "#M = 200\n",
    "M = 300\n",
    "\n",
    "K = 10\n",
    "\n",
    "# initialize w for the gradient ascent\n",
    "W1_init = 2 * np.random.random((M,D)) - 1\n",
    "W2_init = 2 * np.random.random((K,M+1)) - 1\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0.01\n",
    "\n",
    "# options for gradient descent\n",
    "options = [250, 1e-6, 0.5/N]\n",
    "\n",
    "'''\n",
    "gradEw1, gradEw2, numericalGrad1, numericalGrad2 = gradcheck_softmax(W1_init, W2_init, X_train, y_train, lamda)\n",
    "\n",
    "# Absolute norm\n",
    "print( \"The difference estimate for gradient of w1 is : \", np.max(np.abs(gradEw1 - numericalGrad1)) )\n",
    "print( \"The difference estimate for gradient of w2 is : \", np.max(np.abs(gradEw2 - numericalGrad2)) )\n",
    "'''\n",
    "\n",
    "# Train the model\n",
    "W1, W2, costs = ml_softmax_train(y_train, X_train, lamda, W1_init, W2_init, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(format(options[2], 'f')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predict </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_test(W1, W2, X_test):\n",
    "    ytest = softmax( act_func(choice, X_test.dot(W1.T)).dot((W2[:,1:]).T))\n",
    "    # Hard classification decisions\n",
    "    ttest = np.argmax(ytest, 1)\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = ml_softmax_test(W1, W2, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Accuracy </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean( pred == np.argmax(y_test,1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at our test data. Check out some of the misclassified test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faults = np.where(np.not_equal(np.argmax(y_test,1),pred))[0]\n",
    "# plot n misclassified examples from the Test set\n",
    "n = 25\n",
    "samples = np.random.choice(faults, n)\n",
    "sqrt_n = int( n ** 0.5 )\n",
    "plt.figure( figsize=(11,13) )\n",
    "\n",
    "cnt = 0\n",
    "for i in samples:\n",
    "    cnt += 1\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt )\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n",
    "    plt.imshow( X_test[i,1:].reshape(28,28)*255, cmap='gray' )\n",
    "    plt.title(\"True: \"+str(np.argmax(y_test,1)[i])+ \"\\n Predicted: \"+ str(pred[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
